{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/studiomd2025/notebooks/blob/main/medleyvox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcT1h4Z4yOpA"
      },
      "source": [
        "# Colab Inference for MedleyVox\n",
        "\n",
        "Medley Vox is a [dataset for testing algorithms for separating multiple singers](https://arxiv.org/pdf/2211.07302) within a single music track. Also, the [authors of Medley Vox](https://github.com/jeonchangbin49/MedleyVox) proposed a neural network architecture for separating singers. However, unfortunately, they did not publish the weights. Later, their training process was [repeated by Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main), who trained several models and published the weights in the public domain. Now this WebUI is created to use the trained models and weights for inference. Here are some precautions:\n",
        "1. Put the [downloaded models](https://huggingface.co/Cyru5/MedleyVox) in the 'checkpoints' folder in folder format, with each model folder containing a model file (.pth) and its corresponding configuration file (.json).\n",
        "2. If you use overlapadd and the choice of model is 'w2v' or 'w2v_chunk', you need to download the pretrained model [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) and put it in the 'pretrained' folder.\n",
        "3. At present, the audio output sampling rate supported by the model is 24000kHz and cannot be changed. To solve this, you can use [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution), [Apollo](https://github.com/JusperLee/Apollo), or [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) for audio super-resolution.\n",
        "4. When using WebUI on cloud platforms or Colab, please place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. The 'Select folder' and 'Open folder' buttons are invalid in the cloud.\n",
        "5. If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use 'use_overlapadd'. Among the 'use_overlapadd' options, \"ola\", \"ola_norm\", and \"w2v\" all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with 'vad_method' options spec and webrtc when using either of the \"_chunk\" methods. Chunking has proven to be very useful therefore it is on by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUNrX80oyOpL"
      },
      "source": [
        "# Initialize environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sELXJcNxyOpO"
      },
      "outputs": [],
      "source": [
        "# @title Clone repository and install requirements {\"display-mode\":\"form\"}\n",
        "#@markdown # Clone repository and install requirements\n",
        "#@markdown\n",
        "\n",
        "!nvidia-smi\n",
        "!git clone https://github.com/SUC-DriverOld/MedleyVox-Inference-WebUI\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python -m pip install --upgrade pip==24.0 setuptools\n",
        "!python -m pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu124\n",
        "!mkdir -p inputs\n",
        "!mkdir -p results\n",
        "\n",
        "# Create the checkpoints directory\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoints\"\n",
        "\n",
        "# Download and move vocals_238\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoints/vocals_238\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoints/vocals_238\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/vocals%20238/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/vocals%20238/vocals.json\n",
        "\n",
        "# Download and move multi_singing_librispeech_138\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoints/multi_singing_librispeech_138\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoints/multi_singing_librispeech_138\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/multi_singing_librispeech_138/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/multi_singing_librispeech_138/vocals.json\n",
        "\n",
        "# Download and move singing_librispeech_ft_iSRNet\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoints/singing_librispeech_ft_iSRNet\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoints/singing_librispeech_ft_iSRNet\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/singing_librispeech_ft_iSRNet/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/singing_librispeech_ft_iSRNet/vocals.json\n",
        "\n",
        "# Download pretrained model\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/pretrained\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/pretrained\"\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt\n",
        "\n",
        "%cd /content/MedleyVox-Inference-WebUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBL0LG97yOpT"
      },
      "source": [
        "# Inference\n",
        "\n",
        "### Place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. There are two ways to tun inference: use WebUI or use command line.\n",
        "\n",
        "- Use WebUI: Run the WebUI startup code block and then access the WebUI through the public link.\n",
        "- Use command line: Select appropriate inference parameters and run the the command line code block.\n",
        "\n",
        "### Explanation of reasoning parameters. For more infrmation, refer to `inference.py`.\n",
        "\n",
        "- `Model name`: Select which model you want to use.\n",
        "- `Use overlapadd`: Use overlapadd functions, ola, ola_norm, w2v will work with ola_window_len, ola_hop_len argugments. w2v_chunk and sf_chunk is chunk-wise processing based on VAD, so you have to specify the vad_method args. If you use sf_chunk (spectral_featrues_chunk), you also need to specify spectral_features.\n",
        "- `Separate storage`: Save results in separate folders with the same name as the input file.\n",
        "- `Output format`: Select the output format of the results.\n",
        "- `VAD method`: What method do you want to use for 'voice activity detection (vad) -- split chunks -- processing. Only valid when 'w2v_chunk' or 'sf_chunk' for args.use_overlapadd.\n",
        "- `Spectral features`: What spectral feature do you want to use in correlation calc in speaker assignment (only valid when using sf_chunk)\n",
        "- `OLA window length`: OLA window size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).\n",
        "- `OLA hop length`: OLA hop size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).\n",
        "- `Wav2Vec nth layer output`: Wav2Vec nth layer output, only valid when using w2v or w2v_chunk. For example: 0 1 2 3, default: 0\n",
        "- `Use EMA model`: Use EMA model or online model? Only vaind when args.ema it True (model trained with EMA).\n",
        "- `Mix consistent output`: Only valid when the model is trained with mixture_consistency loss.\n",
        "- `Reorder chunks`: OLA reorder chunks. Only valid when using ola or ola_norm.\n",
        "- `Skip error files`: Skip error files while separating instead of stopping.\n",
        "\n",
        "If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use `use_overlapadd`. Among the `use_overlapadd` options, \"ola\", \"ola_norm\", and \"w2v\" all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with `vad_method` options spec and webrtc when using either of the \"_chunk\" methods. Chunking has proven to be very useful therefore it is on by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7KnZQyDyOpW"
      },
      "outputs": [],
      "source": [
        "# @title Run inference in WebUI {\"display-mode\":\"form\"}\n",
        "#@markdown # Run inference in WebUI\n",
        "#@markdown\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Language Setting\n",
        "language = \"English\" #@param [\"English\", \"简体中文\"]\n",
        "\n",
        "import os\n",
        "language_dict = {\"English\": \"en_US\", \"简体中文\": \"zh_CN\"}\n",
        "os.environ[\"LANGUAGE\"] = language_dict[language]\n",
        "\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python webui.py -s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGyz-doYyOpY"
      },
      "outputs": [],
      "source": [
        "# @title Run inference in Command Line {\"display-mode\":\"form\"}\n",
        "#@markdown # Run inference in Command Line\n",
        "#@markdown\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown File and model Parameters\n",
        "folder_input = \"inputs\" #@param {type:\"string\"}\n",
        "store_dir = \"results\" #@param {type:\"string\"}\n",
        "model_name = \"multi_singing_librispeech_138\" #@param [\"vocals_238\", \"multi_singing_librispeech_138\", \"singing_librispeech_ft_iSRNet\"]\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Common Parameters\n",
        "use_overlapadd = \"ola\" #@param [\"None\", \"ola\", \"ola_norm\", \"w2v\", \"w2v_chunk\", \"sf_chunk\"]\n",
        "separate_storage = True #@param {type:\"boolean\"}\n",
        "skip_error = True #@param {type:\"boolean\"}\n",
        "output_format = \"wav\" #@param [\"wav\", \"flac\", \"mp3\"]\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Advanced Parameters\n",
        "vad_method = \"spec\" #@param [\"spec\", \"webrtc\"]\n",
        "spectral_features = \"mfcc\" #@param [\"mfcc\", \"spectral_centroid\"]\n",
        "ola_window_len = \"0\" #@param {type:\"string\"}\n",
        "ola_hop_len = \"0\" #@param {type:\"string\"}\n",
        "w2v_nth_layer_output = \"0\" #@param {type:\"string\"}\n",
        "use_ema_model = True #@param {type:\"boolean\"}\n",
        "mix_consistent_out = True #@param {type:\"boolean\"}\n",
        "reorder_chunks = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "MODEL_DIR = \"checkpoint\"\n",
        "PRETRAINED_MODEL_DIR = \"pretrained\"\n",
        "use_gpu = True\n",
        "\n",
        "model_file = os.path.basename(glob.glob(os.path.join(MODEL_DIR, model_name, \"*.pth\"))[0])\n",
        "target = model_file.replace(\".pth\", \"\")\n",
        "exp_name = model_name\n",
        "model_dir = MODEL_DIR\n",
        "params = f\"--target \\\"{target}\\\" --exp_name \\\"{exp_name}\\\" --model_dir \\\"{model_dir}\\\"\"\n",
        "if use_gpu:\n",
        "    params += \" --use_gpu y\"\n",
        "else:\n",
        "    params += \" --use_gpu n\"\n",
        "if use_overlapadd != \"None\":\n",
        "    params += f\" --use_overlapadd {use_overlapadd}\"\n",
        "params += f\" --vad_method {vad_method} --spectral_features {spectral_features} --w2v_ckpt_dir {PRETRAINED_MODEL_DIR} --w2v_nth_layer_output {w2v_nth_layer_output}\"\n",
        "if ola_window_len != \"0\":\n",
        "    params += f\" --ola_window_len {ola_window_len}\"\n",
        "if ola_hop_len != \"0\":\n",
        "    params += f\" --ola_hop_len {ola_hop_len}\"\n",
        "if use_ema_model:\n",
        "    params += \" --use_ema_model y\"\n",
        "else:\n",
        "    params += \" --use_ema_model n\"\n",
        "if mix_consistent_out:\n",
        "    params += \" --mix_consistent_out y\"\n",
        "else:\n",
        "    params += \" --mix_consistent_out n\"\n",
        "if reorder_chunks:\n",
        "    params += \" --reorder_chunks y\"\n",
        "else:\n",
        "    params += \" --reorder_chunks n\"\n",
        "if skip_error:\n",
        "    params += \" --skip_error y\"\n",
        "else:\n",
        "    params += \" --skip_error n\"\n",
        "if separate_storage:\n",
        "    params += f\" --separate_storage y\"\n",
        "else:\n",
        "    params += f\" --separate_storage n\"\n",
        "params += f\" --output_format {output_format} --inference_data_dir \\\"{folder_input}\\\" --results_save_dir \\\"{store_dir}\\\"\"\n",
        "print(params)\n",
        "\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python inference.py {params}"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}